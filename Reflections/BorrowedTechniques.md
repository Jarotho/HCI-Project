# Ubiquitous computing

Ubiquitous computing is a type of technology that allows us to interact with it in a natural way and use it wherever and whenever we need it. When a person uses it, they are not thinking about the device they are using, but rather the action they need to perform. A great example of this is Alexa, Ok Google, Siri, and even watches that provide information about our physical activity. There are countless products that apply this concept.

Ubiquitous computing can be defined as a subset of HCI (Human-Computer Interaction), as these technologies allow the user to interact with the world rather than the technology itself. In the past, HCI projects focused on creating screens, menus, and elements that were good for the user. Nowadays, ubiquitous computing projects focus more on the user's context and how technology interacts with the environment. There is no distinction between HCI and ubiquitous computing; both seek to help people interact with technology in a better way.

Lastly, it's important to discuss the advantages and disadvantages of ubiquitous computing. While these technologies are widely useful in medical fields, offering tools to detect certain conditions early on, they can also be used as sensors in other work areas. They do not require extensive programming work, and the costs are often low. However, these technologies have a significant security problem, as security is not often considered when creating these projects. Technology is advancing, and ubiquitous computing is a great example of how it can be used to improve human-computer interaction, offering a great step towards the future.


### References

- Y. (2021, November 18). What is Ubiquitous Computing? YoungWonks.https://www.youngwonks.com/blog/What-is-Ubiquitous-Computing
-Gurdin, B. (2014). What is the relationship between Ubiquitous Computing and Human Computer Interaction? Quora. https://www.quora.com/What-is-the-relationship-between-Ubiquitous-Computing-and-Human-Computer-Interaction/answer/Boaz-Gurdin
